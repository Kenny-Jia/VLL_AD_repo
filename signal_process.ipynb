{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Deep learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Scikit-learn for preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_objects_vectorized(e_feats, p_feats):\n",
    "    # Count electrons with energy > 0\n",
    "    electron_count = np.sum(e_feats[:, :, 0] > 0, axis=1)\n",
    "    \n",
    "    # Count photons with energy > 0\n",
    "    photon_count = np.sum(p_feats[:, :, 0] > 0, axis=1)\n",
    "    \n",
    "    # Total count combines electron and photon counts\n",
    "    total_count = electron_count + photon_count\n",
    "    \n",
    "    # Create mask for events with at least 2 objects\n",
    "    valid_events = total_count >= 2\n",
    "    \n",
    "    return valid_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataReader:\n",
    "    \"\"\"Handles reading and preprocessing of signal physics event data.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, scaler_path):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.scaler_path = Path(scaler_path)\n",
    "\n",
    "        # Load feature lists and scaler parameters\n",
    "        with open(self.scaler_path, 'r') as f:\n",
    "            self.scaler_params = json.load(f)\n",
    "\n",
    "        # Define feature lists based on README\n",
    "        self.electron_features_list = [\n",
    "            'electron_E', 'electron_pt', 'electron_eta', 'electron_phi',\n",
    "            'electron_time',\n",
    "            'electron_d0', 'electron_z0', 'electron_dpt',\n",
    "            'electron_nPIX', 'electron_nMissingLayers',\n",
    "            'electron_chi2', 'electron_numberDoF',  # Will need to handle ratio\n",
    "            'electron_f1', 'electron_f3', 'electron_z'\n",
    "        ]\n",
    "        \n",
    "        self.photon_features_list = [\n",
    "            'photon_E', 'photon_pt', 'photon_eta', 'photon_phi',\n",
    "            'photon_time',\n",
    "            'photon_maxEcell_E',\n",
    "            'photon_f1', 'photon_f3', 'photon_r1', 'photon_r2',\n",
    "            'photon_etas1', 'photon_phis1', 'photon_z'\n",
    "        ]\n",
    "        \n",
    "        # Initialize and load scalers\n",
    "        self._initialize_and_load_scalers()\n",
    "\n",
    "        # Load and preprocess all data\n",
    "        self.load_all_data()\n",
    "\n",
    "    def _initialize_and_load_scalers(self):\n",
    "        \"\"\"Initialize specialized scalers for each feature group based on saved parameters.\"\"\"\n",
    "        print(f\"Loading specialized scalers from {self.scaler_path}...\")\n",
    "        \n",
    "        # Initialize electron scalers and feature groups\n",
    "        self.electron_scalers = {}\n",
    "        self.electron_feature_groups = {}\n",
    "        \n",
    "        for group_name, params in self.scaler_params['electron'].items():\n",
    "            # Get feature indices\n",
    "            self.electron_feature_groups[group_name] = params.get('feature_indices', [])\n",
    "            \n",
    "            # Create the appropriate scaler type based on the saved parameters\n",
    "            scaler_type = params.get('type', 'StandardScaler')\n",
    "            \n",
    "            # Initialize the scaler based on type and load its parameters\n",
    "            self._initialize_scaler(self.electron_scalers, group_name, scaler_type, params)\n",
    "        \n",
    "        # Initialize photon scalers and feature groups\n",
    "        self.photon_scalers = {}\n",
    "        self.photon_feature_groups = {}\n",
    "        \n",
    "        for group_name, params in self.scaler_params['photon'].items():\n",
    "            # Get feature indices\n",
    "            self.photon_feature_groups[group_name] = params.get('feature_indices', [])\n",
    "            \n",
    "            # Create the appropriate scaler type based on the saved parameters\n",
    "            scaler_type = params.get('type', 'StandardScaler')\n",
    "            \n",
    "            # Initialize the scaler based on type and load its parameters\n",
    "            self._initialize_scaler(self.photon_scalers, group_name, scaler_type, params)\n",
    "        \n",
    "        # Initialize vertex scaler (simple case)\n",
    "        vertex_params = self.scaler_params['vertex']\n",
    "        self.vertex_scaler = StandardScaler()\n",
    "        if 'mean' in vertex_params and 'scale' in vertex_params:\n",
    "            self.vertex_scaler.mean_ = np.array(vertex_params['mean'])\n",
    "            self.vertex_scaler.scale_ = np.array(vertex_params['scale'])\n",
    "            self.vertex_scaler.var_ = np.square(self.vertex_scaler.scale_)\n",
    "        \n",
    "        print(\"Scalers loaded successfully.\")\n",
    "\n",
    "    def _initialize_scaler(self, scalers_dict, group_name, scaler_type, params):\n",
    "        \"\"\"Initialize a specific scaler based on its type and parameters.\"\"\"\n",
    "        if scaler_type == 'StandardScaler':\n",
    "            scaler = StandardScaler()\n",
    "            if 'mean' in params and 'scale' in params:\n",
    "                scaler.mean_ = np.array(params['mean'])\n",
    "                scaler.scale_ = np.array(params['scale'])\n",
    "                scaler.var_ = np.square(scaler.scale_)\n",
    "        \n",
    "        elif scaler_type == 'RobustScaler':\n",
    "            scaler = RobustScaler()\n",
    "            if 'center' in params and 'scale' in params:\n",
    "                scaler.center_ = np.array(params['center'])\n",
    "                scaler.scale_ = np.array(params['scale'])\n",
    "        \n",
    "        elif scaler_type == 'MinMaxScaler':\n",
    "            scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "            scaler.min_ = np.array(params[\"min_\"])\n",
    "            scaler.scale_ = np.array(params[\"scale_\"])\n",
    "        \n",
    "        elif scaler_type == 'PowerTransformer':\n",
    "            method = params.get('method', 'yeo-johnson')\n",
    "            standardize = params.get('standardize', True)\n",
    "            scaler = PowerTransformer(method=method, standardize=standardize)\n",
    "            scaler.lambdas_ = np.array(params['lambdas'])\n",
    "\n",
    "            # Restore internal StandardScaler for standardization\n",
    "            if standardize:\n",
    "                scaler._scaler = StandardScaler()\n",
    "                scaler._scaler.mean_ = np.array(params['mean'])\n",
    "                scaler._scaler.scale_ = np.array(params['scale'])\n",
    "            \n",
    "            # Restore internal scaler manually if standardization was used\n",
    "            if standardize and 'mean' in params and 'scale' in params:\n",
    "                scaler._scaler = StandardScaler()\n",
    "                scaler._scaler.mean_ = np.array(params['mean'])\n",
    "                scaler._scaler.scale_ = np.array(params['scale'])\n",
    "        \n",
    "        else:\n",
    "            # Default to StandardScaler if unknown type\n",
    "            print(f\"Warning: Unknown scaler type '{scaler_type}' for {group_name}, using StandardScaler\")\n",
    "        \n",
    "        # Store the initialized scaler\n",
    "        scalers_dict[group_name] = scaler\n",
    "\n",
    "    def load_all_data(self):\n",
    "        \"\"\"Load and preprocess signal data from HDF5 files.\"\"\"\n",
    "        print(\"Loading all signal data...\")\n",
    "        \n",
    "        # Initialize as None for first file\n",
    "        self.electron_features = None\n",
    "        self.photon_features = None\n",
    "        self.vertex_features = None\n",
    "        \n",
    "        file_count = 0\n",
    "        \n",
    "        for file_path in self.data_dir.glob(\"*.h5\"):\n",
    "            with h5py.File(file_path, 'r', rdcc_nbytes=10*1024*1024) as f:\n",
    "                n_events = len(f['events/PV_x'])\n",
    "                print(f\"Processing {file_path.name}: {n_events} events\")\n",
    "                \n",
    "                # Load all data at once\n",
    "                electrons = {feat: f[f'events/electrons/{feat}'][:] for feat in self.electron_features_list}\n",
    "                photons = {feat: f[f'events/photons/{feat}'][:] for feat in self.photon_features_list}\n",
    "                vertices = np.stack([\n",
    "                    f['events/PV_x'][:],\n",
    "                    f['events/PV_y'][:],\n",
    "                    f['events/PV_z'][:]\n",
    "                ], axis=1)\n",
    "                \n",
    "                # Process all events at once - for signal, only filter based on energy\n",
    "                e_mask = (electrons['electron_E'] > 0)\n",
    "                \n",
    "                # Initialize arrays for all events\n",
    "                e_feats = np.zeros((n_events, 4, len(self.electron_features_list)))\n",
    "                p_feats = np.zeros((n_events, 4, len(self.photon_features_list)))\n",
    "                \n",
    "                # Process all events at once\n",
    "                for feat_idx, feat in enumerate(self.electron_features_list):\n",
    "                    e_feats[..., feat_idx] = electrons[feat]\n",
    "                    e_feats[..., feat_idx] = np.where(e_mask, e_feats[..., feat_idx], 0)  # Zero out electrons failing selection\n",
    "                \n",
    "                for feat_idx, feat in enumerate(self.photon_features_list):\n",
    "                    p_feats[..., feat_idx] = photons[feat]\n",
    "\n",
    "                # Apply event filtering: Require at least two objects\n",
    "                electron_count = np.sum(e_feats[:, :, 0] > 0, axis=1)\n",
    "                photon_count = np.sum(p_feats[:, :, 0] > 0, axis=1)\n",
    "                total_count = electron_count + photon_count\n",
    "\n",
    "                # Create mask for events with at least 2 objects\n",
    "                valid_events = total_count >= 2\n",
    "                \n",
    "                # Apply the filter\n",
    "                e_feats = e_feats[valid_events]\n",
    "                p_feats = p_feats[valid_events]\n",
    "                vertices = vertices[valid_events]\n",
    "\n",
    "                # Add to main arrays\n",
    "                if self.electron_features is None:\n",
    "                    self.electron_features = e_feats\n",
    "                    self.photon_features = p_feats\n",
    "                    self.vertex_features = vertices\n",
    "                else:\n",
    "                    self.electron_features = np.concatenate([self.electron_features, e_feats])\n",
    "                    self.photon_features = np.concatenate([self.photon_features, p_feats])\n",
    "                    self.vertex_features = np.concatenate([self.vertex_features, vertices])\n",
    "                \n",
    "                file_count += 1\n",
    "                print(f\"Processed {file_count} files, total events: {len(self.electron_features):,}\")\n",
    "        \n",
    "        print(f\"\\nFinal dataset:\")\n",
    "        print(f\"Total files processed: {file_count}\")\n",
    "        print(f\"Total events: {len(self.electron_features):,}\")\n",
    "        print(f\"Shapes: electrons {self.electron_features.shape}, photons {self.photon_features.shape}, vertices {self.vertex_features.shape}\")\n",
    "        \n",
    "        # Apply saved scalers\n",
    "        print(\"\\nApplying saved scalers to signal data...\")\n",
    "        self._transform_features()\n",
    "        \n",
    "        print(f\"Final dataset: {len(self.electron_features):,} events\")\n",
    "        print(f\"Shapes: electrons {self.electron_features.shape}, photons {self.photon_features.shape}, vertices {self.vertex_features.shape}\")    \n",
    "    \n",
    "    def _transform_features(self):\n",
    "        \"\"\"Transform features using loaded scalers.\"\"\"\n",
    "        # Create working copies\n",
    "        e_feats_transformed = self.electron_features.copy()\n",
    "        p_feats_transformed = self.photon_features.copy()\n",
    "        \n",
    "        # Process electron features by group\n",
    "        for group_name, feature_indices in self.electron_feature_groups.items():\n",
    "            # Get all feature data for this group at once\n",
    "            group_values = np.column_stack([\n",
    "                self.electron_features[:, :, idx].reshape(-1, 1) \n",
    "                for idx in feature_indices\n",
    "            ])\n",
    "            \n",
    "            # Transform all features in the group together\n",
    "            transformed_values = self.electron_scalers[group_name].transform(group_values)\n",
    "            \n",
    "            # Split back into individual features and update\n",
    "            for i, feat_idx in enumerate(feature_indices):\n",
    "                feat_transformed = transformed_values[:, i].reshape(\n",
    "                    self.electron_features.shape[0], self.electron_features.shape[1]\n",
    "                )\n",
    "                e_feats_transformed[:, :, feat_idx] = feat_transformed\n",
    "        \n",
    "        # Process photon features by group\n",
    "        for group_name, feature_indices in self.photon_feature_groups.items():\n",
    "            # Get all feature data for this group at once\n",
    "            group_values = np.column_stack([\n",
    "                self.photon_features[:, :, idx].reshape(-1, 1) \n",
    "                for idx in feature_indices\n",
    "            ])\n",
    "            \n",
    "            # Transform all features in the group together\n",
    "            transformed_values = self.photon_scalers[group_name].transform(group_values)\n",
    "            \n",
    "            # Split back into individual features and update\n",
    "            for i, feat_idx in enumerate(feature_indices):\n",
    "                feat_transformed = transformed_values[:, i].reshape(\n",
    "                    self.photon_features.shape[0], self.photon_features.shape[1]\n",
    "                )\n",
    "                p_feats_transformed[:, :, feat_idx] = feat_transformed\n",
    "        \n",
    "        # For vertices, simple standard scaling\n",
    "        self.vertex_features = self.vertex_scaler.transform(self.vertex_features)\n",
    "        \n",
    "        # Update features with transformed versions\n",
    "        self.electron_features = e_feats_transformed\n",
    "        self.photon_features = p_feats_transformed \n",
    "    \n",
    "    def transform_new_data(self, electron_features, photon_features, vertex_features):\n",
    "        \"\"\"Transform new data using loaded scalers.\"\"\"\n",
    "        # Create working copies\n",
    "        e_feats_transformed = electron_features.copy()\n",
    "        p_feats_transformed = photon_features.copy()\n",
    "        \n",
    "        # Process electron features by group\n",
    "        for group_name, feature_indices in self.electron_feature_groups.items():\n",
    "            # Get all feature data for this group at once\n",
    "            group_values = np.column_stack([\n",
    "                electron_features[:, :, idx].reshape(-1, 1) \n",
    "                for idx in feature_indices\n",
    "            ])\n",
    "            \n",
    "            # Transform all features in the group together\n",
    "            transformed_values = self.electron_scalers[group_name].transform(group_values)\n",
    "            \n",
    "            # Split back into individual features and update\n",
    "            for i, feat_idx in enumerate(feature_indices):\n",
    "                feat_transformed = transformed_values[:, i].reshape(\n",
    "                    electron_features.shape[0], electron_features.shape[1]\n",
    "                )\n",
    "                e_feats_transformed[:, :, feat_idx] = feat_transformed\n",
    "        \n",
    "        # Process photon features by group\n",
    "        for group_name, feature_indices in self.photon_feature_groups.items():\n",
    "            # Get all feature data for this group at once\n",
    "            group_values = np.column_stack([\n",
    "                photon_features[:, :, idx].reshape(-1, 1) \n",
    "                for idx in feature_indices\n",
    "            ])\n",
    "            \n",
    "            # Transform all features in the group together\n",
    "            transformed_values = self.photon_scalers[group_name].transform(group_values)\n",
    "            \n",
    "            # Split back into individual features and update\n",
    "            for i, feat_idx in enumerate(feature_indices):\n",
    "                feat_transformed = transformed_values[:, i].reshape(\n",
    "                    photon_features.shape[0], photon_features.shape[1]\n",
    "                )\n",
    "                p_feats_transformed[:, :, feat_idx] = feat_transformed\n",
    "        \n",
    "        # For vertices, simple standard scaling\n",
    "        v_feats_transformed = self.vertex_scaler.transform(vertex_features)\n",
    "        \n",
    "        return e_feats_transformed, p_feats_transformed, v_feats_transformed\n",
    "\n",
    "    def get_all(self):\n",
    "        \"\"\"Return all processed data as a tuple.\"\"\"\n",
    "        all_data = (\n",
    "            self.electron_features,\n",
    "            self.photon_features,\n",
    "            self.vertex_features\n",
    "        )\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading specialized scalers from /fs/ddn/sdf/group/atlas/d/hjia625/VLL-DP/VLL_classifier/src/output/scaler_params.json...\n",
      "Scalers loaded successfully.\n",
      "Loading all signal data...\n",
      "Processing signal_543784.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297402._000001.trees.h5: 50000 events\n",
      "Processed 1 files, total events: 38,720\n",
      "Processing signal_543785.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297406._000001.trees.h5: 50000 events\n",
      "Processed 2 files, total events: 46,987\n",
      "Processing signal_543828.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297651._000001.trees.h5: 50000 events\n",
      "Processed 3 files, total events: 59,845\n",
      "Processing signal_543822.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297639._000001.trees.h5: 50000 events\n",
      "Processed 4 files, total events: 61,561\n",
      "Processing signal_543832.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297658._000001.trees.h5: 50000 events\n",
      "Processed 5 files, total events: 110,277\n",
      "Processing signal_543790.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297432._000001.trees.h5: 50000 events\n",
      "Processed 6 files, total events: 118,060\n",
      "Processing signal_543815.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297624._000001.trees.h5: 50000 events\n",
      "Processed 7 files, total events: 167,050\n",
      "Processing signal_543831.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297656._000001.trees.h5: 50000 events\n",
      "Processed 8 files, total events: 216,615\n",
      "Processing signal_543783.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297394._000001.trees.h5: 50000 events\n",
      "Processed 9 files, total events: 258,180\n",
      "Processing signal_543830.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297654._000001.trees.h5: 50000 events\n",
      "Processed 10 files, total events: 261,997\n",
      "Processing signal_543789.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297426._000001.trees.h5: 50000 events\n",
      "Processed 11 files, total events: 263,723\n",
      "Processing signal_543798.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297461._000001.trees.h5: 50000 events\n",
      "Processed 12 files, total events: 297,811\n",
      "Processing signal_543802.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297485._000001.trees.h5: 50000 events\n",
      "Processed 13 files, total events: 334,904\n",
      "Processing signal_543808.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297603._000001.trees.h5: 50000 events\n",
      "Processed 14 files, total events: 346,726\n",
      "Processing signal_543803.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297488._000001.trees.h5: 50000 events\n",
      "Processed 15 files, total events: 395,797\n",
      "Processing signal_543790.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297430._000002.trees.h5: 50000 events\n",
      "Processed 16 files, total events: 403,831\n",
      "Processing signal_543815.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297625._000001.trees.h5: 50000 events\n",
      "Processed 17 files, total events: 452,763\n",
      "Processing signal_543799.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297466._000001.trees.h5: 50000 events\n",
      "Processed 18 files, total events: 499,455\n",
      "Processing signal_543792.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297439._000001.trees.h5: 49999 events\n",
      "Processed 19 files, total events: 531,648\n",
      "Processing signal_543800.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297468._000001.trees.h5: 50000 events\n",
      "Processed 20 files, total events: 576,048\n",
      "Processing signal_543805.e8564_e8528_a930_s4114_r15540_r15516_p6069.43297516._000001.trees.h5: 50000 events\n",
      "Processed 21 files, total events: 603,237\n",
      "Processing signal_543824.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297643._000001.trees.h5: 50000 events\n",
      "Processed 22 files, total events: 652,424\n",
      "Processing signal_543784.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297400._000001.trees.h5: 50000 events\n",
      "Processed 23 files, total events: 691,414\n",
      "Processing signal_543838.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297707._000001.trees.h5: 50000 events\n",
      "Processed 24 files, total events: 710,188\n",
      "Processing signal_543797.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297459._000001.trees.h5: 50000 events\n",
      "Processed 25 files, total events: 722,071\n",
      "Processing signal_543816.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297626._000001.trees.h5: 50000 events\n",
      "Processed 26 files, total events: 769,696\n",
      "Processing signal_543812.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297612._000001.trees.h5: 50000 events\n",
      "Processed 27 files, total events: 790,837\n",
      "Processing signal_543798.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297463._000001.trees.h5: 50000 events\n",
      "Processed 28 files, total events: 824,809\n",
      "Processing signal_543831.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297657._000001.trees.h5: 50000 events\n",
      "Processed 29 files, total events: 874,334\n",
      "Processing signal_543832.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297659._000001.trees.h5: 50000 events\n",
      "Processed 30 files, total events: 923,012\n",
      "Processing signal_543793.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297440._000001.trees.h5: 50000 events\n",
      "Processed 31 files, total events: 926,309\n",
      "Processing signal_543813.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297614._000001.trees.h5: 50000 events\n",
      "Processed 32 files, total events: 927,803\n",
      "Processing signal_543794.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297444._000001.trees.h5: 50000 events\n",
      "Processed 33 files, total events: 941,720\n",
      "Processing signal_543829.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297653._000001.trees.h5: 50000 events\n",
      "Processed 34 files, total events: 942,582\n",
      "Processing signal_543809.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297605._000001.trees.h5: 50000 events\n",
      "Processed 35 files, total events: 943,235\n",
      "Processing signal_543806.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297520._000001.trees.h5: 50000 events\n",
      "Processed 36 files, total events: 982,696\n",
      "Processing signal_543826.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297646._000001.trees.h5: 50000 events\n",
      "Processed 37 files, total events: 1,024,025\n",
      "Processing signal_543786.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297417._000001.trees.h5: 49999 events\n",
      "Processed 38 files, total events: 1,050,988\n",
      "Processing signal_543825.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297644._000001.trees.h5: 50000 events\n",
      "Processed 39 files, total events: 1,067,472\n",
      "Processing signal_543808.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297604._000001.trees.h5: 50000 events\n",
      "Processed 40 files, total events: 1,079,104\n",
      "Processing signal_543793.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297442._000001.trees.h5: 50000 events\n",
      "Processed 41 files, total events: 1,082,294\n",
      "Processing signal_543837.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297705._000001.trees.h5: 50000 events\n",
      "Processed 42 files, total events: 1,087,634\n",
      "Processing signal_543803.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297495._000001.trees.h5: 50000 events\n",
      "Processed 43 files, total events: 1,136,661\n",
      "Processing signal_543783.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297180._000001.trees.h5: 50000 events\n",
      "Processed 44 files, total events: 1,178,448\n",
      "Processing signal_543836.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297704._000001.trees.h5: 50000 events\n",
      "Processed 45 files, total events: 1,205,726\n",
      "Processing signal_543785.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297404._000001.trees.h5: 50000 events\n",
      "Processed 46 files, total events: 1,214,203\n",
      "Processing signal_543801.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297483._000001.trees.h5: 50000 events\n",
      "Processed 47 files, total events: 1,232,125\n",
      "Processing signal_543809.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297606._000001.trees.h5: 50000 events\n",
      "Processed 48 files, total events: 1,232,802\n",
      "Processing signal_543819.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297633._000001.trees.h5: 49999 events\n",
      "Processed 49 files, total events: 1,254,708\n",
      "Processing signal_543787.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297421._000001.trees.h5: 50000 events\n",
      "Processed 50 files, total events: 1,294,571\n",
      "Processing signal_543802.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297486._000001.trees.h5: 50000 events\n",
      "Processed 51 files, total events: 1,331,634\n",
      "Processing signal_543821.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297637._000001.trees.h5: 50000 events\n",
      "Processed 52 files, total events: 1,332,061\n",
      "Processing signal_543789.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297428._000001.trees.h5: 50000 events\n",
      "Processed 53 files, total events: 1,333,702\n",
      "Processing signal_543800.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297479._000001.trees.h5: 50000 events\n",
      "Processed 54 files, total events: 1,378,206\n",
      "Processing signal_543817.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297628._000001.trees.h5: 50000 events\n",
      "Processed 55 files, total events: 1,401,006\n",
      "Processing signal_543811.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297610._000001.trees.h5: 50000 events\n",
      "Processed 56 files, total events: 1,441,963\n",
      "Processing signal_543799.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297465._000001.trees.h5: 50000 events\n",
      "Processed 57 files, total events: 1,488,662\n",
      "Processing signal_543836.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297703._000001.trees.h5: 50000 events\n",
      "Processed 58 files, total events: 1,516,441\n",
      "Processing signal_543823.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297641._000001.trees.h5: 50000 events\n",
      "Processed 59 files, total events: 1,566,061\n",
      "Processing signal_543813.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297615._000001.trees.h5: 50000 events\n",
      "Processed 60 files, total events: 1,567,520\n",
      "Processing signal_543819.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297632._000001.trees.h5: 50000 events\n",
      "Processed 61 files, total events: 1,589,844\n",
      "Processing signal_543812.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297613._000001.trees.h5: 50000 events\n",
      "Processed 62 files, total events: 1,610,792\n",
      "Processing signal_543834.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297694._000001.trees.h5: 50000 events\n",
      "Processed 63 files, total events: 1,653,961\n",
      "Processing signal_543817.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297629._000001.trees.h5: 50000 events\n",
      "Processed 64 files, total events: 1,676,662\n",
      "Processing signal_543816.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297627._000001.trees.h5: 50000 events\n",
      "Processed 65 files, total events: 1,724,224\n",
      "Processing signal_543822.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297638._000001.trees.h5: 50000 events\n",
      "Processed 66 files, total events: 1,726,024\n",
      "Processing signal_543804.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297515._000001.trees.h5: 50000 events\n",
      "Processed 67 files, total events: 1,774,191\n",
      "Processing signal_543787.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297419._000001.trees.h5: 50000 events\n",
      "Processed 68 files, total events: 1,814,120\n",
      "Processing signal_543833.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297670._000001.trees.h5: 50000 events\n",
      "Processed 69 files, total events: 1,839,392\n",
      "Processing signal_543807.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297551._000001.trees.h5: 50000 events\n",
      "Processed 70 files, total events: 1,870,027\n",
      "Processing signal_543821.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297636._000001.trees.h5: 50000 events\n",
      "Processed 71 files, total events: 1,870,449\n",
      "Processing signal_543792.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297437._000001.trees.h5: 50000 events\n",
      "Processed 72 files, total events: 1,903,202\n",
      "Processing signal_543797.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297457._000001.trees.h5: 50000 events\n",
      "Processed 73 files, total events: 1,915,272\n",
      "Processing signal_543811.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297611._000001.trees.h5: 50000 events\n",
      "Processed 74 files, total events: 1,955,723\n",
      "Processing signal_543823.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297640._000001.trees.h5: 50000 events\n",
      "Processed 75 files, total events: 2,005,335\n",
      "Processing signal_543827.e8564_e8528_s4277_s4114_r15530_r15514_p6069.43297649._000001.trees.h5: 50000 events\n",
      "Processed 76 files, total events: 2,037,086\n",
      "Processing signal_543834.e8564_e8528_s4237_s4114_r15540_r15516_p6069.43297682._000001.trees.h5: 50000 events\n",
      "Processed 77 files, total events: 2,080,280\n",
      "\n",
      "Final dataset:\n",
      "Total files processed: 77\n",
      "Total events: 2,080,280\n",
      "Shapes: electrons (2080280, 4, 15), photons (2080280, 4, 13), vertices (2080280, 3)\n",
      "\n",
      "Applying saved scalers to signal data...\n",
      "Final dataset: 2,080,280 events\n",
      "Shapes: electrons (2080280, 4, 15), photons (2080280, 4, 13), vertices (2080280, 3)\n"
     ]
    }
   ],
   "source": [
    "# Test data loading and preprocessing\n",
    "data_dir = \"/fs/ddn/sdf/group/atlas/d/hjia625/VLL-DP/VLL_classifier/hdf5_signal_output\"\n",
    "scalar_path = \"/fs/ddn/sdf/group/atlas/d/hjia625/VLL-DP/VLL_classifier/src/output/scaler_params.json\"\n",
    "reader = SignalDataReader(data_dir, scalar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train/val/test splits\n",
    "all_signal = reader.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2080280, 4, 15)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electron_features, photon_features, vertices = all_signal\n",
    "np.array(electron_features).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('signal_data_e.npy', electron_features)\n",
    "np.save('signal_data_p.npy', photon_features)\n",
    "np.save('signal_data_v.npy', vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.54841698e+00  1.54757093e+00 -9.64014450e-01 -1.32190589e+00\n",
      "  -1.40644550e-01  5.41521502e+00 -2.67910218e+00 -2.40094867e-02\n",
      "   1.00000000e+00  0.00000000e+00  2.19650166e-01  1.59134593e+00\n",
      "   5.54905102e+01  2.28443714e-01  1.97551516e-05]\n",
      " [ 1.60289098e+00  1.51562423e+00 -5.71524059e-01 -1.22031674e+00\n",
      "  -3.16543430e-01 -6.61583138e+00 -6.54915314e+01 -5.82080841e-01\n",
      "   1.00000000e+00  0.00000000e+00  2.19650166e-01  1.60533610e+00\n",
      "   4.97479642e+01 -8.80099228e-02 -2.45832784e-03]\n",
      " [ 1.56390265e+00  1.47006406e+00 -1.39032884e+00 -5.30572935e-03\n",
      "  -3.53752375e-02 -2.10090446e+01 -6.62908249e+01 -1.31678909e-01\n",
      "   0.00000000e+00  0.00000000e+00  6.92112628e-01  1.59134593e+00\n",
      "   4.39527342e+01  7.63784909e-01 -4.06860080e-03]\n",
      " [ 1.49462312e+00  1.46598289e+00 -1.18399247e+00 -6.81469054e-01\n",
      "   4.22006011e-01  1.84539413e+01 -7.22690353e+01 -1.24363974e-01\n",
      "   1.00000000e+00  0.00000000e+00  6.92112628e-01  1.58928858e+00\n",
      "   1.75476819e+01  2.94126450e+00 -6.55712443e-03]]\n"
     ]
    }
   ],
   "source": [
    "print(electron_features[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Electrons - shape (1, 4, 15)\n",
    "e_feats = np.array([\n",
    "    [  # Event 0\n",
    "        # Electron 0 (15 features)\n",
    "        [65.2, 55.3, 1.2, 0.5, 1.1, 0.05, 0.15, 0.08, 6, 1, 1.5, 7, 0.45, 0.15, 35.0],\n",
    "        # Electron 1 \n",
    "        [48.7, 42.1, -0.8, 2.1, 0.9, -0.03, 0.08, 0.07, 5, 0, 0.9, 6, 0.52, 0.18, -42.0], \n",
    "        # Electron 2\n",
    "        [37.1, 35.6, 0.5, -1.8, 1.3, 0.02, 0.11, 0.09, 7, 1, 1.2, 8, 0.38, 0.22, 15.0],\n",
    "        # Electron 3 (padding - all zeros)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0, 0.0, 0.0, 0.0]\n",
    "    ],\n",
    "    [  # Event 0\n",
    "        # Electron 0 (15 features)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0, 0.0, 0.0, 0.0],\n",
    "        # Electron 1 \n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0, 0.0, 0.0, 0.0], \n",
    "        # Electron 2\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0, 0.0, 0.0, 0.0],\n",
    "        # Electron 3 (padding - all zeros)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, 0.0, 0, 0.0, 0.0, 0.0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Photons - shape (1, 4, 13)\n",
    "p_feats = np.array([\n",
    "    [  # Event 0\n",
    "        # Photon 0 (13 features)\n",
    "        [72.5, 68.3, 0.7, 1.2, 0.8, 12.5, 0.55, 0.12, 0.83, 0.91, 0.025, 0.015, 24.0],\n",
    "        # Photon 1\n",
    "        [63.1, 61.2, -1.1, -0.5, 0.9, 10.2, 0.48, 0.15, 0.78, 0.87, 0.031, 0.018, -18.0],\n",
    "        # Photon 2 (padding - all zeros)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        # Photon 3 (padding - all zeros)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    ],\n",
    "    [  # Event 0\n",
    "        # Photon 0 (13 features)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        # Photon 1\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        # Photon 2 (padding - all zeros)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        # Photon 3 (padding - all zeros)\n",
    "        [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "    ]\n",
    "])\n",
    "\n",
    "# Vertex - shape (1, 3)\n",
    "v_feats = np.array([\n",
    "    [0.05, -0.03, 12.4], [0.0, 0.0, 0.0]  # x, y, z coordinates\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_feats_transformed, p_feats_transformed, v_feats_transformed = reader.transform_new_data(e_feats, p_feats, v_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1.59097474e+00  1.62705790e+00  4.78493591e-01  1.59154887e-01\n",
      "    1.10000000e+00  5.00000000e-02  1.50000000e-01  8.00000000e-02\n",
      "    2.77477172e+00  2.26772628e+00  1.66822786e+00  1.24570654e+00\n",
      "    2.58125923e+00  2.49537196e+01  4.48486359e-02]\n",
      "  [ 1.56164954e+00  1.60462253e+00 -3.21643342e-01  6.68450768e-01\n",
      "    9.00000000e-01 -3.00000000e-02  8.00000000e-02  7.00000000e-02\n",
      "    2.21405888e+00 -3.00438871e-01  1.51182443e+00  1.19477551e+00\n",
      "    3.07491851e+00  3.00278616e+01 -5.17347392e-02]\n",
      "  [ 1.52884413e+00  1.58833992e+00  1.98445664e-01 -5.72957942e-01\n",
      "    1.30000000e+00  2.00000000e-02  1.10000000e-01  9.00000000e-02\n",
      "    3.33548457e+00  2.26772628e+00  1.61353262e+00  1.28644253e+00\n",
      "    2.08759994e+00  3.67933842e+01  1.97620449e-02]\n",
      "  [-6.34503288e-01 -6.34618939e-01 -1.58856875e-03 -7.58910003e-08\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   -5.89505351e-01 -3.00438871e-01 -6.33984016e-01 -6.34877193e-01\n",
      "   -5.92264747e-01 -4.16990332e-01  9.47101732e-04]]\n",
      "\n",
      " [[-6.34503288e-01 -6.34618939e-01 -1.58856875e-03 -7.58910003e-08\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   -5.89505351e-01 -3.00438871e-01 -6.33984016e-01 -6.34877193e-01\n",
      "   -5.92264747e-01 -4.16990332e-01  9.47101732e-04]\n",
      "  [-6.34503288e-01 -6.34618939e-01 -1.58856875e-03 -7.58910003e-08\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   -5.89505351e-01 -3.00438871e-01 -6.33984016e-01 -6.34877193e-01\n",
      "   -5.92264747e-01 -4.16990332e-01  9.47101732e-04]\n",
      "  [-6.34503288e-01 -6.34618939e-01 -1.58856875e-03 -7.58910003e-08\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   -5.89505351e-01 -3.00438871e-01 -6.33984016e-01 -6.34877193e-01\n",
      "   -5.92264747e-01 -4.16990332e-01  9.47101732e-04]\n",
      "  [-6.34503288e-01 -6.34618939e-01 -1.58856875e-03 -7.58910003e-08\n",
      "    0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   -5.89505351e-01 -3.00438871e-01 -6.33984016e-01 -6.34877193e-01\n",
      "   -5.92264747e-01 -4.16990332e-01  9.47101732e-04]]] [[[ 1.88099060e+00  1.42505796e+00  9.96542503e-01  9.96125134e-01\n",
      "    8.00000000e-01 -2.89381918e-01  3.98899546e+00  1.41896037e+01\n",
      "   -5.31205786e-01 -5.30287227e-01  2.55821928e-02  1.35056076e-02\n",
      "    6.23508633e-04]\n",
      "  [ 1.87621540e+00  1.24249277e+00  9.92947648e-01  9.92732400e-01\n",
      "    9.00000000e-01 -2.90633709e-01  3.41873076e+00  1.78041775e+01\n",
      "   -5.31286656e-01 -5.30348149e-01  3.16150133e-02  1.60311988e-02\n",
      "   -9.29160089e-04]\n",
      "  [-5.39373403e-01 -8.61472875e-03  9.95144503e-01  9.93730263e-01\n",
      "    0.00000000e+00 -2.98219163e-01 -4.91655752e-01 -2.68691556e-01\n",
      "   -5.32548238e-01 -5.31673218e-01  4.45440510e-04  8.77651686e-04\n",
      "   -2.63730637e-04]\n",
      "  [-5.39373403e-01 -8.61472875e-03  9.95144503e-01  9.93730263e-01\n",
      "    0.00000000e+00 -2.98219163e-01 -4.91655752e-01 -2.68691556e-01\n",
      "   -5.32548238e-01 -5.31673218e-01  4.45440510e-04  8.77651686e-04\n",
      "   -2.63730637e-04]]\n",
      "\n",
      " [[-5.39373403e-01 -8.61472875e-03  9.95144503e-01  9.93730263e-01\n",
      "    0.00000000e+00 -2.98219163e-01 -4.91655752e-01 -2.68691556e-01\n",
      "   -5.32548238e-01 -5.31673218e-01  4.45440510e-04  8.77651686e-04\n",
      "   -2.63730637e-04]\n",
      "  [-5.39373403e-01 -8.61472875e-03  9.95144503e-01  9.93730263e-01\n",
      "    0.00000000e+00 -2.98219163e-01 -4.91655752e-01 -2.68691556e-01\n",
      "   -5.32548238e-01 -5.31673218e-01  4.45440510e-04  8.77651686e-04\n",
      "   -2.63730637e-04]\n",
      "  [-5.39373403e-01 -8.61472875e-03  9.95144503e-01  9.93730263e-01\n",
      "    0.00000000e+00 -2.98219163e-01 -4.91655752e-01 -2.68691556e-01\n",
      "   -5.32548238e-01 -5.31673218e-01  4.45440510e-04  8.77651686e-04\n",
      "   -2.63730637e-04]\n",
      "  [-5.39373403e-01 -8.61472875e-03  9.95144503e-01  9.93730263e-01\n",
      "    0.00000000e+00 -2.98219163e-01 -4.91655752e-01 -2.68691556e-01\n",
      "   -5.32548238e-01 -5.31673218e-01  4.45440510e-04  8.77651686e-04\n",
      "   -2.63730637e-04]]] [[68.04743153 11.23733972  0.47573748]\n",
      " [62.79853436 12.76125738  0.10543163]]\n"
     ]
    }
   ],
   "source": [
    "print(e_feats_transformed, p_feats_transformed, v_feats_transformed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
